{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acebcf8",
   "metadata": {},
   "source": [
    "# NER Tagging Notebook \n",
    "\n",
    "This notebook takes the preprocessed legal clause data and generates BILOC tags for every clause type.\n",
    "\n",
    "### Script Section - Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0fd4775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a706f41",
   "metadata": {},
   "source": [
    "### Loading Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddad6071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads in json object containing all clauses per document and clause type, and their respective locations(s)\n",
    "with open(\"./data/rft_clauses_final.json\", \"r\") as file:\n",
    "    rft_clauses_final = json.load(file)\n",
    "    \n",
    "# Loads list of abbreviated clause type names:\n",
    "with open(\"./data/clause_tag_names.ob\", \"rb\") as fp:\n",
    "    ctn = pickle.load(fp)\n",
    "    \n",
    "# Loads in list of legal documents in string format:   \n",
    "with open(\"./data/docs_in_str.ob\", 'rb') as fp:\n",
    "    docs_in_str = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5203d49",
   "metadata": {},
   "source": [
    "### Abbreviated Clause Name (Tag) and Index Dictionaries Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c720ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of ner_tag_index and ner_index_tag dictionaries\n",
    "tags = [\"OUTSIDE\"]\n",
    "BILOU = \"BILU\"\n",
    "for i, tag in enumerate(ctn):\n",
    "    for j, anotate_type in enumerate(BILOU):\n",
    "        tags.append(anotate_type + '-' + tag)\n",
    "\n",
    "ner_index_tag = {}\n",
    "\n",
    "for i, ner_tag in enumerate(tags):\n",
    "    ner_index_tag[i] = ner_tag\n",
    "    \n",
    "ner_tag_index = dict((v,k) for k,v in ner_index_tag.items())\n",
    "\n",
    "with open(\"./data/ner_tag_index.ob\", 'wb') as fp:\n",
    "    pickle.dump(ner_tag_index, fp)\n",
    "    \n",
    "with open(\"./data/ner_index_tag.ob\", 'wb') as fp: \n",
    "    pickle.dump(ner_index_tag, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a33ef7c",
   "metadata": {},
   "source": [
    "### Document Split Tokenization and NER Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "249685ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe to hold ner_tags and split_tokens from documents\n",
    "tagged_data_df = pd.DataFrame(columns=['id', 'ner_tags', 'split_tokens'])\n",
    "\n",
    "# Number of Documents to Parse and Tag\n",
    "N = len(rft_clauses_final)\n",
    "\n",
    "for i in range(N):\n",
    "    # Tokenized Text Documents\n",
    "    # Get string form or text document from list\n",
    "    text_document = docs_in_str[i]\n",
    "    \n",
    "    # Skip document if content == \"ERROR\"\n",
    "    # File could not be found or read previously\n",
    "    if text_document == \"ERROR\":\n",
    "        continue\n",
    "    \n",
    "    # Instantiation of Token Tagging Assisting Datastructures\n",
    "    location_heap = []\n",
    "    clause_instances = {}\n",
    "    clause_list = []\n",
    "    \n",
    "    # Dictionary of location of clauses within text file indexed by type\n",
    "    doc_clause_locations = rft_clauses_final[str(i)][\"locations\"]\n",
    "    # Dictionary of clauses found within document indexed by type\n",
    "    doc_clauses = rft_clauses_final[str(i)][\"clauses\"]\n",
    "    # Clause instance counter:\n",
    "    c_inst = 0\n",
    "    \n",
    "    # Iterates over every type of clause within datset:\n",
    "    # Populates Token Tagging Assisting Datastructures \n",
    "    for clause_type in ctn:\n",
    "        # List of clauses in document of type clause_type\n",
    "        clause_type_samples = doc_clauses[clause_type]\n",
    "        \n",
    "        # Move to next type if there are no examples:\n",
    "        if len(clause_type_samples) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Boolean on whether clause has whitespace; used later in tagging: \n",
    "        has_ws = True\n",
    "        \n",
    "        # Iterate over each clause_type sample:\n",
    "        for clause in clause_type_samples:\n",
    "            # Add indexed clause to document clause list\n",
    "            clause_list.append((clause, clause_type))\n",
    "            c_index = len(clause_list) - 1\n",
    "            \n",
    "            # Check for whitespace in clause:\n",
    "            if clause.find(' ') == -1:\n",
    "                has_ws = False\n",
    "            \n",
    "            # Location list of clauses within text:\n",
    "            clause_locations = doc_clause_locations[clause_type][clause]\n",
    "            \n",
    "            # Iterate over each location of clause:\n",
    "            for loc in clause_locations:\n",
    "                # Location_heap tuple -> (start_index,end_index,instance_key,has_whitespace)\n",
    "                clause_loc_tuple = (loc[0],loc[1],c_inst,has_ws)\n",
    "                location_heap.append(clause_loc_tuple)\n",
    "                \n",
    "                # Add instance to clause_instances dictionary with index to its sample:\n",
    "                clause_instances[c_inst] = c_index\n",
    "                \n",
    "                # Increase clause instance count by one:\n",
    "                c_inst += 1\n",
    "            \n",
    "            # Resets has_whitespace for next clause\n",
    "            has_ws = True\n",
    "    \n",
    "    # Heapify list of clause location tuples\n",
    "    heapq.heapify(location_heap)     \n",
    "    \n",
    "    # Instantiate list for tokenizing text document:\n",
    "    tokenized_doc = []\n",
    "    ner_tags = []\n",
    "    temp = []\n",
    "    \n",
    "    doc_char_index = 0\n",
    "    for j in range(c_inst):\n",
    "        loc_info = heapq.heappop(location_heap)\n",
    "        start, end, inst_key, has_ws = loc_info\n",
    "        \n",
    "        # Outside Tagging:\n",
    "        section = text_document[doc_char_index:start].split()\n",
    "        tags = [0] * len(section)\n",
    "        tokenized_doc.extend(section)\n",
    "        ner_tags.extend(tags)\n",
    "        \n",
    "        # Clause Tagging:\n",
    "        section = text_document[start:end].split()\n",
    "        clause_type = clause_list[clause_instances[inst_key]][1]\n",
    "        if has_ws:\n",
    "            # Beginning Tag:\n",
    "            tags = [ner_tag_index[\"B-\" + clause_type]]\n",
    "            # Inner Tags:\n",
    "            tags.extend([ner_tag_index[\"I-\" + clause_type]] * (len(section) - 2))\n",
    "            # Last Tag:\n",
    "            tags.extend([ner_tag_index[\"L-\" + clause_type]])\n",
    "            \n",
    "            doc_char_index = end\n",
    "        else:\n",
    "            # Unit Tagging:\n",
    "            # If surrounding characters of a unit tag are not whitespace\n",
    "            # search function picked up a \"clause\" within another word.\n",
    "            # Thus no words should get tokenized or tags added.\n",
    "            if (end == len(text_document) or text_document[end]) == ' ' and (start == 0 or text_document[start-1] == ' '):\n",
    "                tags = [ner_tag_index[\"U-\" + clause_type]]\n",
    "                doc_char_index = end\n",
    "            else:\n",
    "                section = []\n",
    "                tags = []\n",
    "                \n",
    "                # If false clause is the start of another clause set doc_char_index \n",
    "                # to start_index to prevent a duplication of tokenization and tagging\n",
    "                if len(location_heap) != 0 and start == location_heap[0][0]:\n",
    "                    doc_char_index = start\n",
    "                \n",
    "        # Adding Clause Tags:\n",
    "        tokenized_doc.extend(section)\n",
    "        ner_tags.extend(tags)\n",
    "        \n",
    "        temp.append(loc_info)\n",
    "        if len(tokenized_doc) != len(ner_tags):\n",
    "            print(i)\n",
    "            break\n",
    "            \n",
    "    # Final Outside Tagging\n",
    "    section = text_document[doc_char_index:].split()\n",
    "    tags = [0] * len(section)\n",
    "    tokenized_doc.extend(section)\n",
    "    ner_tags.extend(tags)\n",
    "    \n",
    "    # Print warning if the # of tokens match the # of tags:\n",
    "    if len(tokenized_doc) != len(ner_tags):\n",
    "        print(\"Warning Token Tag Number Mismatch. Document:\", i)\n",
    "    \n",
    "    # Add tokenized document and respective NER tags to dataframe\n",
    "    tagged_data_df.loc[len(tagged_data_df.index)] = [i, ner_tags, tokenized_doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0841e88e",
   "metadata": {},
   "source": [
    "### Saving of Tokenized and Tagged Documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e2675de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to json file\n",
    "tagged_data_df.to_json(\"./data/biloc_tagged_clauses.json\", orient=\"table\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1de7db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save clause class tags to json file\n",
    "feature_class_labels = list(ner_tag_index.keys())\n",
    "with open(\"./data/feature_class_labels.json\", 'w') as f:\n",
    "    json.dump(feature_class_labels, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
