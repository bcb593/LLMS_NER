{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da7d73b-5082-4797-a844-860f5ec3a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.nn.functional import softmax\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import numpy as np\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "from transformers import TrainerCallback, AdamW, get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "148b417d-169f-4f41-a195-a0c679010cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b20fbeb-8b72-4df6-bb42-8119fe78f7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ner_tags', 'id', 'split_tokens'],\n",
      "        num_rows: 401\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['ner_tags', 'id', 'split_tokens'],\n",
      "        num_rows: 71\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data_files ='../Data/biloc_tagged_clauses.json'\n",
    "datasets = load_dataset('json', data_files=data_files, field='data')\n",
    "test_size=0.15\n",
    "random_seed=42\n",
    "\n",
    "datasets = datasets['train'].train_test_split(test_size=test_size, seed=random_seed)\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ed96d9-23e6-4179-b1f1-773bee42e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "lr = 1e-3\n",
    "batch_size = 16\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8da93d7d-2ad2-4a79-8fa6-0f50b4ea43a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "141a5261-9c88-4c4f-a0d7-d9e1b094bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/feature_class_labels.json', 'r') as f:\n",
    "    label_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07fa369b-2cc9-43af-92c6-c3d5335d8807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [[id2label[p] for p, l in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    true_labels = [[id2label[l] for p, l in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    # Assuming you have a seqeval wrapper or configuration that accepts scheme specification\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c133234-344d-43cd-ac6e-cf8e6752440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff725bb2-1de5-4f0f-8c85-a758127073a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"split_tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b47c871-068c-4a5e-9689-ef7033da4b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7ee1c7ce454fedb6873eb7b649132b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/71 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = datasets.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32405b7a-3c35-4b38-a877-ab658625c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {id: label for id, label in enumerate(label_list)}\n",
    "\n",
    "label2id = {label: id for id, label in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93b49fde-6c65-4e4e-956a-b049199e027d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, num_labels=165, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54ad0820-3a02-4389-840c-aac2d56949aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ef6af4e-9cc7-434d-8d74-c0c605495ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS, inference_mode=False, r=16, lora_alpha=16, lora_dropout=0.1, bias=\"all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8234edd5-3901-4e89-bf0e-a6c58627efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cdf00ab2-f19b-4045-a19d-2562818c5152",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_wnut_model\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",  \n",
    "    load_best_model_at_end=False,  \n",
    "    push_to_hub=False,  \n",
    "    logging_dir=\"./logs\",  \n",
    "    logging_steps=10,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "080959d5-941c-4151-901b-130ece65bca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [900/900 10:05, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.292123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.869968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.355900</td>\n",
       "      <td>0.840726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.869968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.864100</td>\n",
       "      <td>0.825046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.869968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.809400</td>\n",
       "      <td>0.815332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.869968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.795781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.869968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.743500</td>\n",
       "      <td>0.766719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.869968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.751600</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.869968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.648800</td>\n",
       "      <td>0.649634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.870167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.617800</td>\n",
       "      <td>0.599668</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.009604</td>\n",
       "      <td>0.015253</td>\n",
       "      <td>0.873827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.509500</td>\n",
       "      <td>0.531869</td>\n",
       "      <td>0.299546</td>\n",
       "      <td>0.237695</td>\n",
       "      <td>0.265060</td>\n",
       "      <td>0.881782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.509500</td>\n",
       "      <td>0.499307</td>\n",
       "      <td>0.310690</td>\n",
       "      <td>0.334934</td>\n",
       "      <td>0.322357</td>\n",
       "      <td>0.884686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.467800</td>\n",
       "      <td>0.476107</td>\n",
       "      <td>0.292634</td>\n",
       "      <td>0.348139</td>\n",
       "      <td>0.317982</td>\n",
       "      <td>0.885720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.444400</td>\n",
       "      <td>0.463478</td>\n",
       "      <td>0.339138</td>\n",
       "      <td>0.490996</td>\n",
       "      <td>0.401177</td>\n",
       "      <td>0.887709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.393700</td>\n",
       "      <td>0.452370</td>\n",
       "      <td>0.342809</td>\n",
       "      <td>0.492197</td>\n",
       "      <td>0.404140</td>\n",
       "      <td>0.889340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.443903</td>\n",
       "      <td>0.345059</td>\n",
       "      <td>0.494598</td>\n",
       "      <td>0.406512</td>\n",
       "      <td>0.887749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.338300</td>\n",
       "      <td>0.428792</td>\n",
       "      <td>0.364135</td>\n",
       "      <td>0.477791</td>\n",
       "      <td>0.413292</td>\n",
       "      <td>0.890175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.329100</td>\n",
       "      <td>0.440425</td>\n",
       "      <td>0.341425</td>\n",
       "      <td>0.523409</td>\n",
       "      <td>0.413270</td>\n",
       "      <td>0.886158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.348100</td>\n",
       "      <td>0.423233</td>\n",
       "      <td>0.352688</td>\n",
       "      <td>0.393758</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.895267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.299600</td>\n",
       "      <td>0.424372</td>\n",
       "      <td>0.398570</td>\n",
       "      <td>0.535414</td>\n",
       "      <td>0.456967</td>\n",
       "      <td>0.896181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.305300</td>\n",
       "      <td>0.413161</td>\n",
       "      <td>0.384559</td>\n",
       "      <td>0.627851</td>\n",
       "      <td>0.476972</td>\n",
       "      <td>0.895664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.305300</td>\n",
       "      <td>0.404658</td>\n",
       "      <td>0.388571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.462585</td>\n",
       "      <td>0.894988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.282700</td>\n",
       "      <td>0.408671</td>\n",
       "      <td>0.396200</td>\n",
       "      <td>0.675870</td>\n",
       "      <td>0.499556</td>\n",
       "      <td>0.895465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.400934</td>\n",
       "      <td>0.393634</td>\n",
       "      <td>0.608643</td>\n",
       "      <td>0.478076</td>\n",
       "      <td>0.895147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.254700</td>\n",
       "      <td>0.407721</td>\n",
       "      <td>0.410591</td>\n",
       "      <td>0.642257</td>\n",
       "      <td>0.500936</td>\n",
       "      <td>0.891408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.249000</td>\n",
       "      <td>0.396676</td>\n",
       "      <td>0.424149</td>\n",
       "      <td>0.657863</td>\n",
       "      <td>0.515765</td>\n",
       "      <td>0.895346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.251700</td>\n",
       "      <td>0.401933</td>\n",
       "      <td>0.417365</td>\n",
       "      <td>0.657863</td>\n",
       "      <td>0.510718</td>\n",
       "      <td>0.894988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.236800</td>\n",
       "      <td>0.399935</td>\n",
       "      <td>0.428350</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0.520716</td>\n",
       "      <td>0.898608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.226600</td>\n",
       "      <td>0.393275</td>\n",
       "      <td>0.418699</td>\n",
       "      <td>0.618247</td>\n",
       "      <td>0.499273</td>\n",
       "      <td>0.898966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.215400</td>\n",
       "      <td>0.394081</td>\n",
       "      <td>0.402226</td>\n",
       "      <td>0.607443</td>\n",
       "      <td>0.483979</td>\n",
       "      <td>0.897414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.205200</td>\n",
       "      <td>0.401298</td>\n",
       "      <td>0.413609</td>\n",
       "      <td>0.649460</td>\n",
       "      <td>0.505371</td>\n",
       "      <td>0.899960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.205200</td>\n",
       "      <td>0.409657</td>\n",
       "      <td>0.430590</td>\n",
       "      <td>0.621849</td>\n",
       "      <td>0.508841</td>\n",
       "      <td>0.902466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.208200</td>\n",
       "      <td>0.413220</td>\n",
       "      <td>0.436104</td>\n",
       "      <td>0.585834</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.904614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.407372</td>\n",
       "      <td>0.426840</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.495976</td>\n",
       "      <td>0.902864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.187900</td>\n",
       "      <td>0.410913</td>\n",
       "      <td>0.437601</td>\n",
       "      <td>0.648259</td>\n",
       "      <td>0.522496</td>\n",
       "      <td>0.902904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.184400</td>\n",
       "      <td>0.404704</td>\n",
       "      <td>0.430052</td>\n",
       "      <td>0.597839</td>\n",
       "      <td>0.500251</td>\n",
       "      <td>0.900716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.187700</td>\n",
       "      <td>0.400881</td>\n",
       "      <td>0.439831</td>\n",
       "      <td>0.623049</td>\n",
       "      <td>0.515648</td>\n",
       "      <td>0.902506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.192300</td>\n",
       "      <td>0.422703</td>\n",
       "      <td>0.444079</td>\n",
       "      <td>0.648259</td>\n",
       "      <td>0.527086</td>\n",
       "      <td>0.903779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.170400</td>\n",
       "      <td>0.418786</td>\n",
       "      <td>0.436137</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.529051</td>\n",
       "      <td>0.901790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.162400</td>\n",
       "      <td>0.410928</td>\n",
       "      <td>0.443218</td>\n",
       "      <td>0.674670</td>\n",
       "      <td>0.534983</td>\n",
       "      <td>0.900040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.166800</td>\n",
       "      <td>0.409289</td>\n",
       "      <td>0.438918</td>\n",
       "      <td>0.603842</td>\n",
       "      <td>0.508338</td>\n",
       "      <td>0.904336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.166800</td>\n",
       "      <td>0.417531</td>\n",
       "      <td>0.440246</td>\n",
       "      <td>0.601441</td>\n",
       "      <td>0.508371</td>\n",
       "      <td>0.904574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.161300</td>\n",
       "      <td>0.425134</td>\n",
       "      <td>0.448505</td>\n",
       "      <td>0.648259</td>\n",
       "      <td>0.530191</td>\n",
       "      <td>0.902904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.159100</td>\n",
       "      <td>0.419845</td>\n",
       "      <td>0.434276</td>\n",
       "      <td>0.626651</td>\n",
       "      <td>0.513022</td>\n",
       "      <td>0.902904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.150700</td>\n",
       "      <td>0.422060</td>\n",
       "      <td>0.442171</td>\n",
       "      <td>0.596639</td>\n",
       "      <td>0.507920</td>\n",
       "      <td>0.903500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.150300</td>\n",
       "      <td>0.421360</td>\n",
       "      <td>0.432904</td>\n",
       "      <td>0.685474</td>\n",
       "      <td>0.530669</td>\n",
       "      <td>0.901034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.153400</td>\n",
       "      <td>0.415636</td>\n",
       "      <td>0.447627</td>\n",
       "      <td>0.656663</td>\n",
       "      <td>0.532360</td>\n",
       "      <td>0.901432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.422075</td>\n",
       "      <td>0.455833</td>\n",
       "      <td>0.656663</td>\n",
       "      <td>0.538121</td>\n",
       "      <td>0.904057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.144200</td>\n",
       "      <td>0.422155</td>\n",
       "      <td>0.448248</td>\n",
       "      <td>0.660264</td>\n",
       "      <td>0.533981</td>\n",
       "      <td>0.902267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.426225</td>\n",
       "      <td>0.475662</td>\n",
       "      <td>0.668667</td>\n",
       "      <td>0.555888</td>\n",
       "      <td>0.904495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.137600</td>\n",
       "      <td>0.437759</td>\n",
       "      <td>0.470248</td>\n",
       "      <td>0.683073</td>\n",
       "      <td>0.557024</td>\n",
       "      <td>0.903858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.137600</td>\n",
       "      <td>0.443205</td>\n",
       "      <td>0.466490</td>\n",
       "      <td>0.635054</td>\n",
       "      <td>0.537875</td>\n",
       "      <td>0.904495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.134600</td>\n",
       "      <td>0.416492</td>\n",
       "      <td>0.445876</td>\n",
       "      <td>0.623049</td>\n",
       "      <td>0.519780</td>\n",
       "      <td>0.905131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.433480</td>\n",
       "      <td>0.471869</td>\n",
       "      <td>0.624250</td>\n",
       "      <td>0.537468</td>\n",
       "      <td>0.904813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.132400</td>\n",
       "      <td>0.420674</td>\n",
       "      <td>0.483395</td>\n",
       "      <td>0.629052</td>\n",
       "      <td>0.546688</td>\n",
       "      <td>0.907597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.131500</td>\n",
       "      <td>0.440274</td>\n",
       "      <td>0.470790</td>\n",
       "      <td>0.657863</td>\n",
       "      <td>0.548823</td>\n",
       "      <td>0.905927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.129100</td>\n",
       "      <td>0.432021</td>\n",
       "      <td>0.474043</td>\n",
       "      <td>0.668667</td>\n",
       "      <td>0.554781</td>\n",
       "      <td>0.906523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.123300</td>\n",
       "      <td>0.428003</td>\n",
       "      <td>0.447471</td>\n",
       "      <td>0.690276</td>\n",
       "      <td>0.542965</td>\n",
       "      <td>0.901114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.133800</td>\n",
       "      <td>0.417691</td>\n",
       "      <td>0.463537</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0.545903</td>\n",
       "      <td>0.904733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.119900</td>\n",
       "      <td>0.430620</td>\n",
       "      <td>0.473413</td>\n",
       "      <td>0.662665</td>\n",
       "      <td>0.552276</td>\n",
       "      <td>0.904018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.120100</td>\n",
       "      <td>0.431339</td>\n",
       "      <td>0.471586</td>\n",
       "      <td>0.667467</td>\n",
       "      <td>0.552684</td>\n",
       "      <td>0.905529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.120100</td>\n",
       "      <td>0.431116</td>\n",
       "      <td>0.457391</td>\n",
       "      <td>0.631453</td>\n",
       "      <td>0.530509</td>\n",
       "      <td>0.905370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.116000</td>\n",
       "      <td>0.442900</td>\n",
       "      <td>0.477462</td>\n",
       "      <td>0.686675</td>\n",
       "      <td>0.563269</td>\n",
       "      <td>0.904614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.115300</td>\n",
       "      <td>0.440238</td>\n",
       "      <td>0.491023</td>\n",
       "      <td>0.656663</td>\n",
       "      <td>0.561890</td>\n",
       "      <td>0.907120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>0.430470</td>\n",
       "      <td>0.459109</td>\n",
       "      <td>0.680672</td>\n",
       "      <td>0.548356</td>\n",
       "      <td>0.905489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.116800</td>\n",
       "      <td>0.438728</td>\n",
       "      <td>0.463960</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.905131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.114800</td>\n",
       "      <td>0.441460</td>\n",
       "      <td>0.475874</td>\n",
       "      <td>0.686675</td>\n",
       "      <td>0.562162</td>\n",
       "      <td>0.903779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.448697</td>\n",
       "      <td>0.487374</td>\n",
       "      <td>0.695078</td>\n",
       "      <td>0.572984</td>\n",
       "      <td>0.906683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.114400</td>\n",
       "      <td>0.432292</td>\n",
       "      <td>0.471254</td>\n",
       "      <td>0.649460</td>\n",
       "      <td>0.546189</td>\n",
       "      <td>0.905569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>0.445795</td>\n",
       "      <td>0.476596</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.557769</td>\n",
       "      <td>0.907637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.113200</td>\n",
       "      <td>0.443724</td>\n",
       "      <td>0.491803</td>\n",
       "      <td>0.648259</td>\n",
       "      <td>0.559296</td>\n",
       "      <td>0.906961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.113200</td>\n",
       "      <td>0.449354</td>\n",
       "      <td>0.482574</td>\n",
       "      <td>0.648259</td>\n",
       "      <td>0.553279</td>\n",
       "      <td>0.907836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>0.437393</td>\n",
       "      <td>0.471979</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.545823</td>\n",
       "      <td>0.907200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.448818</td>\n",
       "      <td>0.477637</td>\n",
       "      <td>0.679472</td>\n",
       "      <td>0.560951</td>\n",
       "      <td>0.907717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.102100</td>\n",
       "      <td>0.441772</td>\n",
       "      <td>0.465079</td>\n",
       "      <td>0.703481</td>\n",
       "      <td>0.559962</td>\n",
       "      <td>0.905609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.107200</td>\n",
       "      <td>0.427807</td>\n",
       "      <td>0.463968</td>\n",
       "      <td>0.687875</td>\n",
       "      <td>0.554159</td>\n",
       "      <td>0.905052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.105700</td>\n",
       "      <td>0.438902</td>\n",
       "      <td>0.477537</td>\n",
       "      <td>0.689076</td>\n",
       "      <td>0.564128</td>\n",
       "      <td>0.906762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.103700</td>\n",
       "      <td>0.444539</td>\n",
       "      <td>0.469522</td>\n",
       "      <td>0.684274</td>\n",
       "      <td>0.556913</td>\n",
       "      <td>0.903779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.097400</td>\n",
       "      <td>0.448082</td>\n",
       "      <td>0.477329</td>\n",
       "      <td>0.695078</td>\n",
       "      <td>0.565982</td>\n",
       "      <td>0.904773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.108700</td>\n",
       "      <td>0.442921</td>\n",
       "      <td>0.483165</td>\n",
       "      <td>0.689076</td>\n",
       "      <td>0.568036</td>\n",
       "      <td>0.907279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>0.445622</td>\n",
       "      <td>0.493739</td>\n",
       "      <td>0.662665</td>\n",
       "      <td>0.565864</td>\n",
       "      <td>0.906802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>0.445856</td>\n",
       "      <td>0.474874</td>\n",
       "      <td>0.680672</td>\n",
       "      <td>0.559447</td>\n",
       "      <td>0.906842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.103700</td>\n",
       "      <td>0.449707</td>\n",
       "      <td>0.473154</td>\n",
       "      <td>0.677071</td>\n",
       "      <td>0.557037</td>\n",
       "      <td>0.906961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.451069</td>\n",
       "      <td>0.478669</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.559601</td>\n",
       "      <td>0.907558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.098800</td>\n",
       "      <td>0.450564</td>\n",
       "      <td>0.474635</td>\n",
       "      <td>0.662665</td>\n",
       "      <td>0.553106</td>\n",
       "      <td>0.906881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.097400</td>\n",
       "      <td>0.447109</td>\n",
       "      <td>0.478548</td>\n",
       "      <td>0.696279</td>\n",
       "      <td>0.567237</td>\n",
       "      <td>0.907001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>0.449193</td>\n",
       "      <td>0.481643</td>\n",
       "      <td>0.661465</td>\n",
       "      <td>0.557410</td>\n",
       "      <td>0.906961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>0.458185</td>\n",
       "      <td>0.479220</td>\n",
       "      <td>0.678271</td>\n",
       "      <td>0.561630</td>\n",
       "      <td>0.907319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.098500</td>\n",
       "      <td>0.456282</td>\n",
       "      <td>0.482264</td>\n",
       "      <td>0.685474</td>\n",
       "      <td>0.566187</td>\n",
       "      <td>0.906285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.093900</td>\n",
       "      <td>0.452858</td>\n",
       "      <td>0.487424</td>\n",
       "      <td>0.674670</td>\n",
       "      <td>0.565962</td>\n",
       "      <td>0.907200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.097700</td>\n",
       "      <td>0.458222</td>\n",
       "      <td>0.492615</td>\n",
       "      <td>0.680672</td>\n",
       "      <td>0.571573</td>\n",
       "      <td>0.908115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.097700</td>\n",
       "      <td>0.450954</td>\n",
       "      <td>0.481356</td>\n",
       "      <td>0.681873</td>\n",
       "      <td>0.564332</td>\n",
       "      <td>0.907160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.096300</td>\n",
       "      <td>0.454799</td>\n",
       "      <td>0.484147</td>\n",
       "      <td>0.678271</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.907597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.097000</td>\n",
       "      <td>0.458890</td>\n",
       "      <td>0.484282</td>\n",
       "      <td>0.684274</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>0.907359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>0.453069</td>\n",
       "      <td>0.488754</td>\n",
       "      <td>0.678271</td>\n",
       "      <td>0.568125</td>\n",
       "      <td>0.907796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.452911</td>\n",
       "      <td>0.491859</td>\n",
       "      <td>0.689076</td>\n",
       "      <td>0.574000</td>\n",
       "      <td>0.907757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.092800</td>\n",
       "      <td>0.452432</td>\n",
       "      <td>0.487784</td>\n",
       "      <td>0.695078</td>\n",
       "      <td>0.573267</td>\n",
       "      <td>0.907001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.099100</td>\n",
       "      <td>0.451703</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.684274</td>\n",
       "      <td>0.569146</td>\n",
       "      <td>0.907001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.090600</td>\n",
       "      <td>0.451080</td>\n",
       "      <td>0.483816</td>\n",
       "      <td>0.681873</td>\n",
       "      <td>0.566019</td>\n",
       "      <td>0.907041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.094600</td>\n",
       "      <td>0.452998</td>\n",
       "      <td>0.483461</td>\n",
       "      <td>0.684274</td>\n",
       "      <td>0.566600</td>\n",
       "      <td>0.906961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>0.453555</td>\n",
       "      <td>0.483898</td>\n",
       "      <td>0.685474</td>\n",
       "      <td>0.567312</td>\n",
       "      <td>0.906881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=900, training_loss=0.24910887817541758, metrics={'train_runtime': 606.5957, 'train_samples_per_second': 66.107, 'train_steps_per_second': 1.484, 'total_flos': 1.05817304659968e+16, 'train_loss': 0.24910887817541758, 'epoch': 100.0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d76dd47-51da-49c7-b538-1f7b7d0b5074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.45355498790740967,\n",
       " 'eval_precision': 0.48389830508474574,\n",
       " 'eval_recall': 0.6854741896758704,\n",
       " 'eval_f1': 0.5673124689518132,\n",
       " 'eval_accuracy': 0.9068814638027048,\n",
       " 'eval_runtime': 0.9781,\n",
       " 'eval_samples_per_second': 72.59,\n",
       " 'eval_steps_per_second': 2.045,\n",
       " 'epoch': 100.0}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a458adbe-1938-49b1-937f-be184087b112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 109735242\n",
      "Trainable Parameters: 818853\n",
      "Non-trainable Parameters: 108916389\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "non_trainable_params = total_params - trainable_params\n",
    "\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "print(f\"Trainable Parameters: {trainable_params}\")\n",
    "print(f\"Non-trainable Parameters: {non_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e13a5e9-0f05-44da-b6d3-23d391be5a86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
