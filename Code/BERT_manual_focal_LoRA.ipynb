{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bac23c1",
   "metadata": {},
   "source": [
    "# BERT Model: Manual Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d99910b",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed02bf1-be04-44e6-8e30-f455a534d20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\john\\AppData\\Local\\Temp/ipykernel_7688/2972420923.py:6: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "C:\\Users\\john\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.autonotebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4e4838",
   "metadata": {},
   "source": [
    "#### Data Loading and Preperation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dffa2b59-db14-493d-89e1-c374f764cdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'ner_tags', 'split_tokens'],\n",
      "        num_rows: 8646\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'ner_tags', 'split_tokens'],\n",
      "        num_rows: 1526\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from json file:\n",
    "data_file_path ='./data/biloc_tagged_sequences.json'\n",
    "datasets = load_dataset('json', data_files=data_file_path, field='data')\n",
    "\n",
    "# Paramters for dataset train-test-split function: \n",
    "# Sets train-test split and seed of data shuffle\n",
    "test_size=0.15\n",
    "random_seed=42\n",
    "\n",
    "# Split dataset into train and test sets:\n",
    "datasets = datasets['train'].train_test_split(test_size=test_size, seed=random_seed)\n",
    "print(\"Dataset Structure:\")\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b11f92b",
   "metadata": {},
   "source": [
    "#### Tokenize Data for BERT Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d72e652e-c04b-4ae3-a379-97a525f61cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in BERT tokenizer bert-base-cased:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d33cb812-f289-4a6e-a367-111733cf99cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deals with special tokens and ensures correct label alignment:\n",
    "# Helps with tokenization due to dataset format\n",
    "def tokenize_and_align_labels(tokenizer, examples):\n",
    "    \n",
    "    tokenized_inputs = tokenizer(examples[\"split_tokens\"], truncation=True, padding=\"max_length\", \n",
    "                                 is_split_into_words=True, return_tensors=\"pt\")\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = [-100 if word_id is None else label[word_id] for word_id in word_ids]\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Converts batch input to tensor\n",
    "def convert_to_tensors(batch):\n",
    "    batch_tensors = {key: tensor(value) for key, value in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000f834d-5028-4384-8eef-885a51a43709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Dataset\n",
    "tokenized_datasets = datasets.map(lambda examples: tokenize_and_align_labels(tokenizer, examples), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a872b2cd",
   "metadata": {},
   "source": [
    "#### Tokenized Dataset Formatting for Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be5cacb7-1291-4b40-9393-6c888d0d4c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dataset for use with Pytorch:\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'token_type_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4c95b90-bad1-473d-a880-e3da6115a9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pytorch DataLoader Objects for Train and Test Sets:\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71998b17",
   "metadata": {},
   "source": [
    "#### Load in pre-trained BERT Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed62c1e-c7b8-410c-9af8-f50f9aa104bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters:\n",
    "model_name = \"bert-base-cased\"  \n",
    "num_labels = 165\n",
    "\n",
    "# Loads in default model from HuggingFace:\n",
    "bert_model = AutoModel.from_pretrained(model_name, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea796ab1",
   "metadata": {},
   "source": [
    "#### Model, Fine-tuner, and Optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2731059e-0ef5-4879-8774-4dfc84661f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inherits from pytorch.nn.module to add custom fine-tuning to model:\n",
    "class CustomNERModel(nn.Module):\n",
    "    def __init__(self, bert_model, num_labels):\n",
    "        super(CustomNERModel, self).__init__()\n",
    "        self.bert = bert_model  # The BERT model\n",
    "        self.classifier = nn.Linear(bert_model.config.hidden_size, num_labels)  # Classifier\n",
    "        self.config = bert_model.config\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b119deed-367d-4f2d-bc7c-fe533f1f0f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA to the model via PEFT if Boolean True:\n",
    "add_LoRA = True\n",
    "\n",
    "if add_LoRA:\n",
    "    target_modules = [\"query\", \"value\"]\n",
    "    peft_config = LoraConfig(task_type=TaskType.FEATURE_EXTRACTION, inference_mode=False, \n",
    "                             r=16, lora_alpha=8, lora_dropout=0.1, bias=\"all\",\n",
    "                             target_modules=target_modules)\n",
    "\n",
    "    model = get_peft_model(bert_model, peft_config)\n",
    "else:\n",
    "    model = bert_model\n",
    "\n",
    "# Initialization of custom fine-tuned BERT model\n",
    "model = CustomNERModel(model, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca9e9695-1d56-4a83-9bf2-4987eef53adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization of Optimizer and Loss Function:\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4e41d27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set model device to gpu if available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f4007",
   "metadata": {},
   "source": [
    "#### Class Weighting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47795b1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assuming you know the total list of classes, including those not present in training\n",
    "total_classes = np.arange(num_labels)  # num_labels is the total number of unique labels you have\n",
    "\n",
    "# Use only the labels present in the training dataset to calculate class weights\n",
    "present_labels = [label for sublist in datasets[\"train\"][\"ner_tags\"] for label in sublist]\n",
    "present_labels_unique = np.unique(present_labels)\n",
    "\n",
    "# Compute class weights only for present classes\n",
    "class_weights = compute_class_weight('balanced', classes=present_labels_unique, y=present_labels)\n",
    "\n",
    "# Initialize a weights array with ones (default weight) for all classes\n",
    "weights_array = np.ones(num_labels)\n",
    "\n",
    "# Update the weights array for the present classes with the computed weights\n",
    "weights_array[present_labels_unique] = class_weights\n",
    "\n",
    "# Convert to a tensor\n",
    "class_weights_tensor = torch.tensor(weights_array, dtype=torch.float).to(device)\n",
    "\n",
    "# Use this tensor in your loss function\n",
    "loss_function = CrossEntropyLoss(weight=class_weights_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b9453e",
   "metadata": {},
   "source": [
    "#### Focal Loss Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df846e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean', num_classes=None, device='cpu'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        if alpha is not None:\n",
    "            # Directly use the alpha tensor passed as an argument\n",
    "            self.alpha = alpha.to(device)\n",
    "        else:\n",
    "            # If no alpha is provided, initialize a uniform alpha tensor\n",
    "            self.alpha = torch.ones(num_classes, device=device) * 0.25\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        targets = targets.type(torch.long)\n",
    "\n",
    "        # Apply alpha factors based on class indices\n",
    "        at = self.alpha[targets]\n",
    "\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = at * (1 - pt) ** self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fef14d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 165  # Your number of classes\n",
    "alpha = torch.ones(num_labels) * 0.5\n",
    "alpha[0] = 0.05\n",
    "focal_loss_function = FocalLoss(alpha=alpha, gamma=2.0, num_classes=num_labels, device=device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940f8ce0",
   "metadata": {},
   "source": [
    "#### Stat-Collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "635eb880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint Function:\n",
    "def checkpoint(model, filename):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    \n",
    "def resume(model, filename):\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    \n",
    "def calculate_accuracy(logits, labels):\n",
    "    \"\"\"\n",
    "    Calculate accuracy by comparing logits vs labels\n",
    "    \"\"\"\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    mask = labels != -100  # Assuming -100 is used to ignore tokens in the labels\n",
    "    correct_predictions = torch.sum((predictions == labels) * mask)\n",
    "    total_relevant_elements = torch.sum(mask)\n",
    "    accuracy = (correct_predictions.float() / total_relevant_elements.float()).item()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dcade8",
   "metadata": {},
   "source": [
    "#### Model Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd04d6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean to run training procedure; otherwise, loads in previous model checkpoint\n",
    "train_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2160abce-c668-4e02-bc27-8fcb6fd21fd9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "avg_acc = []\n",
    "if train_mode:\n",
    "    num_epochs = 25\n",
    "    save_path = './checkpoints/model_focal_large_LoRA'\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            # Load Inputs and Labels from batch\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Get outputs from model:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs\n",
    "            \n",
    "            # Calculate Loss, Backpropogate, update Optimizer:\n",
    "            loss = focal_loss_function(logits.view(-1, num_labels), labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy for each batch and accumulate\n",
    "            batch_accuracy = calculate_accuracy(logits.view(-1, num_labels), labels.view(-1))\n",
    "            total_accuracy += batch_accuracy\n",
    "            \n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "            \n",
    "        # Average accuracy over all batches\n",
    "        avg_accuracy = total_accuracy / len(train_loader)\n",
    "        avg_acc.append(avg_accuracy)\n",
    "        \n",
    "        # Generates Model Checkpoint Every 10 Epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            checkpoint(model, save_path + str(epoch + 1))\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}, Average Accuracy: {avg_accuracy:.4f}\")\n",
    "else:\n",
    "    # Loads in previous model checkpoint\n",
    "    model_path = './checkpoints/model_focal_sampled_LoRA10'\n",
    "    resume(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8b0fb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "print(model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0be57e",
   "metadata": {},
   "source": [
    "#### Model Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd304d23-2cca-4211-8147-9465be2d1265",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels_list = []\n",
    "pred_labels_list = []\n",
    "\n",
    "with torch.no_grad(): \n",
    "    for batch in test_loader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        predictions = predictions.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        \n",
    "        true_labels_list.append(labels)\n",
    "        pred_labels_list.append(predictions)\n",
    "\n",
    "true_labels_flat = np.concatenate(true_labels_list, axis=None)\n",
    "pred_labels_flat = np.concatenate(pred_labels_list, axis=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ea2ab",
   "metadata": {},
   "source": [
    "#### Prediction Analysis: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba1891d5-4ae0-4f04-8f6b-03b39e23be85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7685\n",
      "Precision: 0.0824\n",
      "Recall: 0.1331\n",
      "F1 Score: 0.0839\n",
      "--------------------------------\n",
      "[0.87708948 0.2        0.16666667 0.33581165 0.         0.25742574\n",
      " 0.21276596 0.31976314 0.46459512 0.32982456 0.35326087 0.31658291\n",
      " 0.         0.28571429 0.11232449 0.29487179 0.         0.08333333\n",
      " 0.3238155  0.0212766  0.         0.34245115 0.         0.\n",
      " 0.20509194 0.         0.3853211  0.35035035 0.17085427 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.083986   0.         0.         0.12329969 0.         0.\n",
      " 0.         0.         0.         0.37908497 0.         0.\n",
      " 0.02919708 0.         0.         0.18670395 0.         0.\n",
      " 0.19876204 0.         0.         0.06304729 0.         0.\n",
      " 0.31963678 0.02083333 0.         0.24983097 0.         0.\n",
      " 0.         0.         0.         0.11974808 0.         0.\n",
      " 0.09691961 0.         0.         0.19499124 0.         0.\n",
      " 0.01520913 0.         0.         0.20883326 0.0212766  0.\n",
      " 0.22477064 0.         0.         0.         0.         0.\n",
      " 0.01366354 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.16295464\n",
      " 0.         0.         0.29116906 0.04545455 0.         0.09651899\n",
      " 0.         0.         0.33940214 0.         0.         0.\n",
      " 0.08052588 0.         0.         0.39838365 0.11428571 0.\n",
      " 0.00363636 0.         0.         0.         0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\john\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\john\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "mask = true_labels_flat != -100 \n",
    "true_labels_filtered = true_labels_flat[mask]\n",
    "pred_labels_filtered = pred_labels_flat[mask]\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels_filtered,\n",
    "                                                           pred_labels_filtered,\n",
    "                                                           average=\"macro\")\n",
    "accuracy = accuracy_score(true_labels_filtered, pred_labels_filtered)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels_filtered,\n",
    "                                                           pred_labels_filtered,\n",
    "                                                           zero_division=0)\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f9b363",
   "metadata": {},
   "source": [
    "#### Model Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2103bc4-b1c1-4fcc-8b1f-715af53bcff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 109026981\n",
      "Trainable Parameters: 819621\n",
      "Non-trainable Parameters: 108207360\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "non_trainable_params = total_params - trainable_params\n",
    "\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "print(f\"Trainable Parameters: {trainable_params}\")\n",
    "print(f\"Non-trainable Parameters: {non_trainable_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba2cc25",
   "metadata": {},
   "source": [
    "#### Save True and Predicted Labels for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7e6fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/true_labels.ob\", 'wb') as fp:\n",
    "    pickle.dump(true_labels_filtered, fp)\n",
    "    \n",
    "with open(\"./data/pre_labels.ob\", 'wb') as fp:\n",
    "    pickle.dump(pred_labels_filtered, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
