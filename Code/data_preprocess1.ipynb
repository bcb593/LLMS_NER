{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98623887",
   "metadata": {},
   "source": [
    "# Data Preprocessing Notebook 1\n",
    "\n",
    "This notebook aggregates a portion of the different scripts used to preprocess the CUAD dataset for use in NER token classification via LLM's.  \n",
    "\n",
    "### Script Section 1 - Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87d88cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa6810a",
   "metadata": {},
   "source": [
    "### Clause Type Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e5cc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Clause Preprocessing Complete.\n"
     ]
    }
   ],
   "source": [
    "# Read in clause data from master clause csv file:\n",
    "data = pd.read_csv(\"./data/master_clauses.csv\")\n",
    "\n",
    "# Determine the indexes of clause existence columns (i.e. columns with Yes/No answers)\n",
    "yes_no_column_indexes = []\n",
    "for i in range(83):\n",
    "    if data.iloc[0][i] == \"No\" or data.iloc[0][i] == \"Yes\":\n",
    "        yes_no_column_indexes.append(i)\n",
    "\n",
    "# Create Dataframe of all clause existence columns\n",
    "# Pull column name from Dataframe and convert to list\n",
    "invalid_clause_tags = data.iloc[:, yes_no_column_indexes]\n",
    "columns_to_drop = invalid_clause_tags.columns.to_list()\n",
    "\n",
    "# Drop clause existence columns from original Dataframe\n",
    "data = data.drop(columns_to_drop, axis=1)\n",
    "\n",
    "# Find all Answer columns (columns containing \"Answer\" in title; contain altered text from documents)\n",
    "answer_columns = []\n",
    "for i, col in enumerate(data.columns):\n",
    "    if \"Answer\" in col:\n",
    "        answer_columns.append(col)\n",
    "\n",
    "# Drop Answer column from original Dataframe\n",
    "data = data.drop(answer_columns, axis=1)\n",
    "\n",
    "# Save cleaned Dataframe to csv file.\n",
    "data.to_csv('./data/cleaned_clauses.csv', index=False) \n",
    "\n",
    "print(\"General Clause Preprocessing Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e7b33e",
   "metadata": {},
   "source": [
    "### Clause Type Name and Abbreviated Tags Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8a5b909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Clause Type Name and Abbreviated Tags Processing.\n"
     ]
    }
   ],
   "source": [
    "# Get the clause types to be used from data. Pulls the columns name from data Dataframe\n",
    "# and converts to list ignoring 1st column (filename header)\n",
    "clause_tags = data.columns.to_list()[1:]\n",
    "\n",
    "# Reads in abbreviations of clause types from csv\n",
    "# Converts entries from DataFrame to list\n",
    "clause_tag_names = pd.read_csv(\"./data/clause_tag_names.csv\")\n",
    "ctn = clause_tag_names[\"clause tag name\"].to_list()\n",
    "\n",
    "# Maps the clause type names to their respective abbreviations:\n",
    "clause_tag_map = dict(zip(ctn,clause_tags))\n",
    "\n",
    "# Save the list of abreviated clause type names to binary file:\n",
    "with open(\"./data/clause_tag_names.ob\", 'wb') as fp: \n",
    "    pickle.dump(ctn, fp)\n",
    "    \n",
    "# Save the map of abreviated tags to clause type names to binary file:\n",
    "with open(\"./data/clause_tag_map.ob\", 'wb') as fp: \n",
    "    pickle.dump(clause_tag_map, fp)\n",
    "\n",
    "print(\"Completed Clause Type Name and Abbreviated Tags Processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19c6d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets all current variables within notebook. Done to prevent potential mishaps\n",
    "# when bringing scripts ran in isolation together.\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fcdde8",
   "metadata": {},
   "source": [
    "### Script Section 2 - Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e862a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import ast\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23f25e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER clause-tagging preperation complete!\n"
     ]
    }
   ],
   "source": [
    "# Load in \"cleaned\" clauses csv\n",
    "clauses = pd.read_csv(\"./data/cleaned_clauses.csv\")\n",
    "\n",
    "# Load in shortened clause type tag names:\n",
    "with open(\"./data/clause_tag_names.ob\", \"rb\") as fp:\n",
    "    clause_tag_names = pickle.load(fp)\n",
    "\n",
    "# Instantiation of ready for tagging document-clause dicionary:\n",
    "rft_clauses = {}\n",
    "\n",
    "# Initialization of Constants:\n",
    "# N -> Number of documents (510)\n",
    "N = len(clauses) \n",
    "# k -> Number of clause types (42)\n",
    "k = len(clauses.iloc[0])\n",
    "\n",
    "# Iterates over every document row in cleaned_clauses csv:\n",
    "for i in range(N):\n",
    "    # Loading document row from Dataframe to variable\n",
    "    document_row = clauses.iloc[i]\n",
    "    \n",
    "    # Creation of individual document object\n",
    "    document = {}\n",
    "    # Removes\".pdf\" file type tag from document name and replaces with \".txt\"\n",
    "    doc_filename = document_row.loc[\"Filename\"][:-4] + \".txt\"\n",
    "    # Adds altered document name to document object\n",
    "    document[\"doc_filename\"] = doc_filename\n",
    "    \n",
    "    #Creation of individual clause object\n",
    "    doc_clauses = {}\n",
    "    \n",
    "    # Iterates over every non-yes/no clause type:\n",
    "    for j in range(1,k):\n",
    "        # Generate list of clauses from document clause type entry\n",
    "        # Format of multiple clauses are a string form list\n",
    "        # ast.literal_eval is utilized to conver to actual list\n",
    "        clause_list = ast.literal_eval(clauses.iloc[i][j])\n",
    "        \n",
    "        # Add clause list to document clause dictionary\n",
    "        # Key is shortened clause type name from clause_tag_names -> list(str)\n",
    "        # Clause types start at index 1 in document row; thus j-1 for ctn which starts at index 0\n",
    "        doc_clauses[clause_tag_names[j-1]] = clause_list\n",
    "        \n",
    "    # Add all clause types to document dictionary\n",
    "    document[\"clauses\"] = doc_clauses\n",
    "    \n",
    "    # Add document dictionary to rft_clauses dictionary\n",
    "    doc_id = i\n",
    "    rft_clauses[doc_id] = document\n",
    "\n",
    "# Serializing json\n",
    "json_object = json.dumps(rft_clauses, indent=4)\n",
    " \n",
    "# Write to 'rft_clauses.json'\n",
    "with open(\"./data/rft_clauses.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)\n",
    "\n",
    "print(\"NER clause-tagging preperation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
